{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1f0621-40ce-4cda-b071-2b3521b4ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "BASE_PATH = './../dynaphos-core/'\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from dynaphos.image_processing import sobel_processor, canny_processor\n",
    "from dynaphos.simulator import GaussianSimulator\n",
    "from dynaphos.utils import load_params, load_coordinates_from_yaml, Map\n",
    "from dynaphos.cortex_models import \\\n",
    "    get_visual_field_coordinates_from_cortex_full\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e668568-bbe0-48ba-99c4-9d0d407b710a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = load_params('../dynaphos-core/config/params.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0916a7cc-482a-4122-89cf-c7afc9c43970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EYE_FRAMERATE = 200\n",
    "\n",
    "params['thresholding']['use_threshold'] = False\n",
    "coordinates_cortex = load_coordinates_from_yaml(\n",
    "    '../dynaphos-core/config/grid_coords_dipole_valid.yaml', n_coordinates=1000)\n",
    "coordinates_cortex = Map(*coordinates_cortex)\n",
    "coordinates_visual_field = get_visual_field_coordinates_from_cortex_full(\n",
    "    params['cortex_model'], coordinates_cortex)\n",
    "simulator = GaussianSimulator(params, coordinates_visual_field)\n",
    "resolution = params['run']['resolution']\n",
    "params['run']['fps'] = EYE_FRAMERATE\n",
    "fps = params['run']['fps']\n",
    "\n",
    "device='cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3df12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_detection(frame, method='sobel', sigma = 2, canny_threshold=200):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    ksize = int(sigma // 2 * 6 + 3) # (odd integer > 3 and > 3 * sigma) \n",
    "    blurred = cv2.GaussianBlur(gray, (ksize, ksize), sigma)\n",
    "    if method == 'sobel':\n",
    "        edges = sobel_processor(blurred)\n",
    "    elif method == 'canny':\n",
    "        edges = canny_processor(blurred, canny_threshold // 2,canny_threshold)\n",
    "    else:\n",
    "        raise NotImplementedError(\"choose method='sobel' or method='canny'\")\n",
    "    edges = (edges - edges.min()) / (edges.max()-edges.min()) # normalize to range [0, 1]\n",
    "    return edges\n",
    "\n",
    "def float2CV(img):\n",
    "    \"\"\"converting float img with values in range [0,1] to openCV uint8 format )\"\"\"\n",
    "    return np.clip((255*img),0,255).astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f7c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(): \n",
    "    x_offset, y_offset = resolution[0]//2, resolution[1] //2\n",
    "    padded = np.zeros((height + resolution[1], width + resolution[0]))\n",
    "    \n",
    "    video_frame_idx = 0\n",
    "    gaze_frame_idx = 0\n",
    "\n",
    "\n",
    "    # Video writers \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    frame_writer = cv2.VideoWriter(os.path.join(save_dir,'orig.avi'), fourcc, EYE_FRAMERATE, (width, height), True)\n",
    "    edge_writer = cv2.VideoWriter(os.path.join(save_dir,'edges.avi'), fourcc, EYE_FRAMERATE, (width, height), False)\n",
    "    patch_writer = cv2.VideoWriter(os.path.join(save_dir,'edge_patches.avi'), fourcc, EYE_FRAMERATE, resolution, False)\n",
    "    phs_writer = cv2.VideoWriter(os.path.join(save_dir,'phosphene_patches.avi'), fourcc, EYE_FRAMERATE, resolution, False)\n",
    "    out_writer = cv2.VideoWriter(os.path.join(save_dir,'phosphenes.avi'), fourcc, EYE_FRAMERATE, (width, height), False)\n",
    "    \n",
    "    merged_writer = cv2.VideoWriter(os.path.join(save_dir,'merged.avi'), fourcc, EYE_FRAMERATE, (width*3, height), True)\n",
    "    merged_patch_writer = cv2.VideoWriter(os.path.join(save_dir,'merged_patch.avi'), fourcc, EYE_FRAMERATE, (resolution[0]*2, resolution[1]), False)\n",
    "\n",
    "    # Read video frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    while(cap.isOpened() and ret): \n",
    "\n",
    "        # Do edge-detection and padding\n",
    "        edges = edge_detection(frame)\n",
    "        padded[y_offset:y_offset+height, \n",
    "               x_offset:x_offset+width] = edges\n",
    "\n",
    "\n",
    "\n",
    "        # Read all gaze frames for this video frame\n",
    "        while video_frame_lookup[gaze_frame_idx] == video_frame_idx:\n",
    "\n",
    "            # Gaze-coordinates in original image\n",
    "            x_pos = int(np.round(gaze_x[gaze_frame_idx] * width))\n",
    "            y_pos = int(np.round(gaze_y[gaze_frame_idx] * height))\n",
    "\n",
    "            # Draw circle for gaze location\n",
    "            frame_with_gaze = cv2.circle(frame.copy(), (x_pos, y_pos), radius=x_offset, color=(0,0,150), thickness=2)\n",
    "            edges_with_gaze = cv2.circle(float2CV(edges.copy()), (x_pos, y_pos), radius=x_offset, color=(255,255,255), thickness=2)\n",
    "\n",
    "\n",
    "            # Extract patch from padded image (edges)\n",
    "            patch = padded[y_pos-y_offset:y_pos+y_offset, # Square patch centered at x_pos, y_pos \n",
    "                           x_pos-x_offset:x_pos+x_offset]\n",
    "\n",
    "            # Gaze-coordinates in padded images\n",
    "            x_pos = np.clip(x_pos, 0, width) + x_offset \n",
    "            y_pos = np.clip(y_pos, 0, height) + y_offset \n",
    "\n",
    "            # Create phosphenes from input patch (edges)\n",
    "            with torch.no_grad():\n",
    "                act_mask = torch.from_numpy(patch).to(device)\n",
    "                stim = simulator.sample_receptive_fields(act_mask) * 90e-6\n",
    "                phs = simulator(stim).cpu().numpy()\n",
    "\n",
    "            # Put the phosphenes in an output frame using the gaze-coordinates\n",
    "            output = np.zeros_like(padded)\n",
    "            output[y_pos-y_offset:y_pos+y_offset, \n",
    "                   x_pos-x_offset:x_pos+x_offset] = phs \n",
    "            \n",
    "            output = output[y_offset:y_offset+height,x_offset:x_offset+width] # exclude the padding\n",
    "\n",
    "\n",
    "            \n",
    "            # Write frames to video files \n",
    "            frame_writer.write(frame_with_gaze)\n",
    "            edge_writer.write(edges_with_gaze)\n",
    "            patch_writer.write(float2CV(patch))\n",
    "            phs_writer.write(float2CV(phs))\n",
    "            out_writer.write(float2CV(output)) \n",
    "            \n",
    "            # Write to merged video file\n",
    "            edges_bgr = cv2.cvtColor(edges_with_gaze,cv2.COLOR_GRAY2BGR)\n",
    "            output_bgr = cv2.cvtColor(float2CV(output),cv2.COLOR_GRAY2BGR)\n",
    "            merged = np.concatenate([frame_with_gaze,edges_bgr,output_bgr], axis=1)\n",
    "            merged_writer.write(merged)\n",
    "            merged_patch_writer.write(np.concatenate([float2CV(patch),float2CV(phs)], axis=1))\n",
    "\n",
    "            # load next gaze frame\n",
    "            if gaze_frame_idx < len(gaze_data)-1:\n",
    "                gaze_frame_idx += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # load next video frame\n",
    "        ret, frame = cap.read()\n",
    "        video_frame_idx +=1\n",
    "\n",
    "    cap.release()\n",
    "    edge_writer.release()\n",
    "    patch_writer.release()\n",
    "    phs_writer.release()\n",
    "    out_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab0afc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (0) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m video_frame_lookup \u001b[38;5;241m=\u001b[39m gaze_data\u001b[38;5;241m.\u001b[39mworld_index \u001b[38;5;241m-\u001b[39m gaze_data\u001b[38;5;241m.\u001b[39mworld_index\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m     22\u001b[0m simulator\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 54\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     53\u001b[0m     act_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(patch)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 54\u001b[0m     stim \u001b[38;5;241m=\u001b[39m \u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_receptive_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m90e-6\u001b[39m\n\u001b[1;32m     55\u001b[0m     phs \u001b[38;5;241m=\u001b[39m simulator(stim)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Put the phosphenes in an output frame using the gaze-coordinates\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/jaap/dynaphos-experiments/eye_tracking_demo/./../dynaphos-core/dynaphos/simulator.py:422\u001b[0m, in \u001b[0;36mGaussianSimulator.sample_receptive_fields\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_receptive_fields\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;124;03m\"\"\"Extracts the maximum value of activation mask x within the 'receptive field' of each phosphene\"\"\"\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (0) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "for folder in ['ObjectGrabbing', 'ObjectViewing', 'Walking', 'Cycling']:\n",
    "    # Data Directory\n",
    "    \n",
    "    data_dir = f'../../_Datasets/MobileEyeTracker/{folder}/'\n",
    "    save_dir = f'./out/{folder}'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "    # Load video data\n",
    "    cap = cv2.VideoCapture(os.path.join(data_dir, 'world.mp4'))\n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "    # Load gaze data\n",
    "    gaze_data = pd.read_csv(os.path.join(data_dir, 'gaze_positions.csv'))\n",
    "    gaze_x, gaze_y = gaze_data.norm_pos_x, gaze_data.norm_pos_y\n",
    "    gaze_y = 1-gaze_y # the coordinate origin is bottom left\n",
    "    video_frame_lookup = gaze_data.world_index - gaze_data.world_index.min()\n",
    "    \n",
    "    simulator.reset()\n",
    "    process_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
